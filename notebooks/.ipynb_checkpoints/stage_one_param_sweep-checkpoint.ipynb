{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef283d29-d5f1-4092-8301-785d7c18fe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n",
      "2026-01-26 15:21:29.234719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-26 15:21:29.367831: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-26 15:21:29.409031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-26 15:21:29.698266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-26 15:21:32.904017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import ast\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Data science and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn - preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OrdinalEncoder,\n",
    "    MinMaxScaler,\n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "# Scikit-learn - model selection and evaluation\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    train_test_split,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "# Scikit-learn - metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "# Scikit-learn - ensemble methods\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    IsolationForest,\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor\n",
    ")\n",
    "\n",
    "# Scikit-learn - clustering and decomposition\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Gradient boosting libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434f29c-c9d1-47ef-a002-13a9084615b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets will not be public in this work. \n",
    "# The following procedure can be applied for other systems and heterogeneous HPC applications.\n",
    "# vasp_data = pd.read_parquet('../data/VASP.parquet')\n",
    "# lammps_data = pd.read_parquet('../data/LAMMPS.parquet')\n",
    "# espresso_data = pd.read_parquet('../data/ESPRESSO.parquet')\n",
    "# atlas_data = pd.read_parquet('../data/ATLAS.parquet')\n",
    "# e3sm_data = pd.read_parquet('../data/E3SM.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2285b436-0382-44ad-82f5-bec13b164655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian wandb - wandb sweep\n",
    "import wandb\n",
    "def param_sweep_lgb(input_data, dataset_name):\n",
    "    df = input_data.copy()\n",
    "    train_features = ['User', 'JobName', 'Account', 'Category', 'req_node', 'req_time']\n",
    "    cat_cols = ['User', 'JobName', 'Account', 'Category']\n",
    "    num_cols = ['req_node', 'req_time']\n",
    "    target_features = ['gpu_utilization_max', 'mem_util_max', 'avg_power']\n",
    "\n",
    "    results_by_target = {}\n",
    "    model_names = ['LightGBM']\n",
    "    model_results = {model_name: {} for model_name in model_names}\n",
    "\n",
    "    # Define the sweep configuration\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': 'mae',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'n_estimators': {'min': 100, 'max': 1000},\n",
    "            'learning_rate': {'min': 0.01, 'max': 0.3},\n",
    "            'max_depth': {'min': 3, 'max': 10},\n",
    "            'num_leaves': {'min': 20, 'max': 150},\n",
    "            'min_child_samples': {'min': 5, 'max': 50},\n",
    "            'subsample': {'min': 0.6, 'max': 1.0},\n",
    "            'colsample_bytree': {'min': 0.6, 'max': 1.0},\n",
    "            'reg_alpha': {'min': 0.0, 'max': 1.0},\n",
    "            'reg_lambda': {'min': 0.0, 'max': 1.0}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=f\"{dataset_name}_lgbm_sweep\")\n",
    "    \n",
    "    def train():\n",
    "        with wandb.init() as run:\n",
    "            # Extract hyperparameters from wandb\n",
    "            config = wandb.config\n",
    "\n",
    "            for target_feature in target_features:\n",
    "                df_cleaned = df.dropna(subset=train_features + [target_feature])\n",
    "                X = df_cleaned[train_features].copy()\n",
    "                y = df_cleaned[target_feature].copy()\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "\n",
    "                # Encode categoricals\n",
    "                encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "                X_train.loc[:, cat_cols] = encoder.fit_transform(X_train[cat_cols])\n",
    "                X_test.loc[:, cat_cols] = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "                # Scale numeric features\n",
    "                scaler = StandardScaler()\n",
    "                X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "                X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "                X_train = X_train.astype(np.float32)\n",
    "                X_test = X_test.astype(np.float32)\n",
    "                y_train = y_train.astype(np.float32)\n",
    "                y_test = y_test.astype(np.float32)\n",
    "\n",
    "                # Shift target to avoid gamma issues\n",
    "                shift = 1e-6\n",
    "                y_train_gamma = y_train + shift\n",
    "\n",
    "                # Train LightGBM with sweep hyperparameters\n",
    "                reg_lgb = lgb.LGBMRegressor(\n",
    "                    random_state=42,\n",
    "                    n_estimators=int(config.n_estimators),\n",
    "                    learning_rate=config.learning_rate,\n",
    "                    max_depth=int(config.max_depth),\n",
    "                    num_leaves=int(config.num_leaves),\n",
    "                    min_child_samples=int(config.min_child_samples),\n",
    "                    subsample=config.subsample,\n",
    "                    colsample_bytree=config.colsample_bytree,\n",
    "                    reg_alpha=config.reg_alpha,\n",
    "                    reg_lambda=config.reg_lambda,\n",
    "                    objective='gamma',\n",
    "                    verbosity=-1\n",
    "                )\n",
    "                reg_lgb.fit(X_train, y_train_gamma)\n",
    "                y_pred_lgb = reg_lgb.predict(X_test)\n",
    "\n",
    "                mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "                r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "                acc_lgb = np.mean(np.minimum(y_pred_lgb / (y_test + 1e-8), y_test / (y_pred_lgb + 1e-8)))\n",
    "\n",
    "                # Log metrics to wandb\n",
    "                wandb.log({\n",
    "                    'target': target_feature,\n",
    "                    'mae': mae_lgb,\n",
    "                    'r2': r2_lgb,\n",
    "                    'accuracy': acc_lgb\n",
    "                })\n",
    "\n",
    "                model_results['LightGBM'][target_feature] = {\n",
    "                    'y_pred': y_pred_lgb, 'r2': r2_lgb, 'mae': mae_lgb, 'accuracy': acc_lgb\n",
    "                }\n",
    "    wandb.agent(sweep_id, function=train, count=30)\n",
    "\n",
    "# %%capture\n",
    "# param_sweep_lgb(vasp_data, 'VASP')\n",
    "# param_sweep_lgb(lammps_data, 'LAMMPS')\n",
    "# param_sweep_lgb(chroma_data, 'CHROMA')\n",
    "# param_sweep_lgb(espresso_data, 'ESPRESSO')\n",
    "# param_sweep_lgb(atlas_data, 'ATLAS')\n",
    "# param_sweep_lgb(e3sm_data, 'E3SM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855051eb-64f8-4579-8a38-ccd2f784d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_sweep_gbr(input_data, dataset_name):\n",
    "    df = input_data.copy()\n",
    "    train_features = ['User', 'JobName', 'Account', 'Category', 'req_node', 'req_time']\n",
    "    cat_cols = ['User', 'JobName', 'Account', 'Category']\n",
    "    num_cols = ['req_node', 'req_time']\n",
    "    target_features = ['gpu_utilization_max', 'mem_util_max', 'avg_power']\n",
    "\n",
    "    results_by_target = {}\n",
    "    model_names = ['GradientBoosting']\n",
    "    model_results = {model_name: {} for model_name in model_names}\n",
    "\n",
    "    # Define the sweep configuration\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': 'mae',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'n_estimators': {'min': 100, 'max': 1000},\n",
    "            'learning_rate': {'min': 0.01, 'max': 0.3},\n",
    "            'max_depth': {'min': 2, 'max': 8},\n",
    "            'min_samples_split': {'min': 2, 'max': 20},\n",
    "            'min_samples_leaf': {'min': 1, 'max': 20},\n",
    "            'subsample': {'min': 0.5, 'max': 1.0},\n",
    "            'max_features': {\n",
    "                'values': ['sqrt', 'log2', None]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=f\"{dataset_name}_gbr_sweep\")\n",
    "    \n",
    "    def train():\n",
    "        with wandb.init() as run:\n",
    "            # Extract hyperparameters from wandb\n",
    "            config = wandb.config\n",
    "\n",
    "            for target_feature in target_features:\n",
    "                df_cleaned = df.dropna(subset=train_features + [target_feature])\n",
    "                X = df_cleaned[train_features].copy()\n",
    "                y = df_cleaned[target_feature].copy()\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "\n",
    "                # Encode categoricals\n",
    "                encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "                X_train.loc[:, cat_cols] = encoder.fit_transform(X_train[cat_cols])\n",
    "                X_test.loc[:, cat_cols] = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "                # Scale numeric features\n",
    "                scaler = StandardScaler()\n",
    "                X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "                X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "                X_train = X_train.astype(np.float32)\n",
    "                X_test = X_test.astype(np.float32)\n",
    "                y_train = y_train.astype(np.float32)\n",
    "                y_test = y_test.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "                # Train GradientBoostingRegressor with sweep hyperparameters\n",
    "                reg_gb = GradientBoostingRegressor(\n",
    "                    random_state=42,\n",
    "                    n_estimators=int(config.n_estimators),\n",
    "                    learning_rate=config.learning_rate,\n",
    "                    max_depth=int(config.max_depth),\n",
    "                    min_samples_split=int(config.min_samples_split),\n",
    "                    min_samples_leaf=int(config.min_samples_leaf),\n",
    "                    subsample=config.subsample,\n",
    "                    max_features=config.max_features\n",
    "                )\n",
    "                \n",
    "                reg_gb.fit(X_train, y_train)\n",
    "                y_pred_gb = reg_gb.predict(X_test)\n",
    "\n",
    "                mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "                r2_gb = r2_score(y_test, y_pred_gb)\n",
    "                acc_gb = np.mean(np.minimum(y_pred_gb / (y_test + 1e-8), y_test / (y_pred_gb + 1e-8)))\n",
    "\n",
    "                wandb.log({\n",
    "                    'target': target_feature,\n",
    "                    'mae': mae_gb,\n",
    "                    'r2': r2_gb,\n",
    "                    'accuracy': acc_gb\n",
    "                })\n",
    "\n",
    "                model_results['GradientBoosting'][target_feature] = {\n",
    "                    'y_pred': y_pred_gb, 'r2': r2_gb, 'mae': mae_gb, 'accuracy': acc_gb\n",
    "                }\n",
    "    wandb.agent(sweep_id, function=train, count=30)\n",
    "\n",
    "# %%capture\n",
    "# param_sweep_gbr(vasp_data, 'VASP')\n",
    "# param_sweep_gbr(lammps_data, 'LAMMPS')\n",
    "# param_sweep_gbr(chroma_data, 'CHROMA')\n",
    "# param_sweep_gbr(espresso_data, 'ESPRESSO')\n",
    "# param_sweep_gbr(atlas_data, 'ATLAS')\n",
    "# param_sweep_gbr(e3sm_data, 'E3SM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08d5ec7-91fe-4b9d-8a4c-fcbafc4750e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_sweep_xgb(input_data, dataset_name):\n",
    "    df = input_data.copy()\n",
    "    train_features = ['User', 'JobName', 'Account', 'Category', 'req_node', 'req_time']\n",
    "    cat_cols = ['User', 'JobName', 'Account', 'Category']\n",
    "    num_cols = ['req_node', 'req_time']\n",
    "    target_features = ['gpu_utilization_max', 'mem_util_max', 'avg_power']\n",
    "\n",
    "    results_by_target = {}\n",
    "    model_names = ['XGBoost']\n",
    "    model_results = {model_name: {} for model_name in model_names}\n",
    "\n",
    "    # Define the sweep configuration\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': 'mae',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'n_estimators': {'min': 100, 'max': 1000},\n",
    "            'learning_rate': {'min': 0.01, 'max': 0.3},\n",
    "            'max_depth': {'min': 2, 'max': 10},\n",
    "            'subsample': {'min': 0.5, 'max': 1.0},\n",
    "            'colsample_bytree': {'min': 0.5, 'max': 1.0},\n",
    "            'gamma': {'min': 0.0, 'max': 5.0},\n",
    "            'reg_alpha': {'min': 0.0, 'max': 1.0},\n",
    "            'reg_lambda': {'min': 0.0, 'max': 1.0}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=f\"{dataset_name}_xgb_sweep\")\n",
    "    \n",
    "    def train():\n",
    "        with wandb.init() as run:\n",
    "            # Extract hyperparameters from wandb\n",
    "            config = wandb.config\n",
    "\n",
    "            for target_feature in target_features:\n",
    "                df_cleaned = df.dropna(subset=train_features + [target_feature])\n",
    "                X = df_cleaned[train_features].copy()\n",
    "                y = df_cleaned[target_feature].copy()\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "\n",
    "                # Encode categoricals\n",
    "                encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "                X_train.loc[:, cat_cols] = encoder.fit_transform(X_train[cat_cols])\n",
    "                X_test.loc[:, cat_cols] = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "                # Scale numeric features\n",
    "                scaler = StandardScaler()\n",
    "                X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "                X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "                X_train = X_train.astype(np.float32)\n",
    "                X_test = X_test.astype(np.float32)\n",
    "                y_train = y_train.astype(np.float32)\n",
    "                y_test = y_test.astype(np.float32)\n",
    "\n",
    "                shift = 1e-6 \n",
    "                y_train_gamma = y_train + shift\n",
    "\n",
    "                # Train XGBRegressor with sweep hyperparameters\n",
    "                reg_xgb = xgb.XGBRegressor(\n",
    "                    random_state=42,\n",
    "                    n_estimators=int(config.n_estimators),\n",
    "                    learning_rate=config.learning_rate,\n",
    "                    max_depth=int(config.max_depth),\n",
    "                    subsample=config.subsample,\n",
    "                    colsample_bytree=config.colsample_bytree,\n",
    "                    gamma=config.gamma,\n",
    "                    reg_alpha=config.reg_alpha,\n",
    "                    reg_lambda=config.reg_lambda,\n",
    "                    objective='reg:gamma',\n",
    "                    tree_method='hist'\n",
    "                )\n",
    "                                \n",
    "                reg_xgb.fit(X_train, y_train_gamma)\n",
    "                y_pred_xgb = reg_xgb.predict(X_test)\n",
    "\n",
    "                mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "                r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "                acc_xgb = np.mean(np.minimum(y_pred_xgb / (y_test + 1e-8), y_test / (y_pred_xgb + 1e-8)))\n",
    "                \n",
    "                wandb.log({\n",
    "                    'target': target_feature,\n",
    "                    'mae': mae_xgb,\n",
    "                    'r2': r2_xgb,\n",
    "                    'accuracy': acc_xgb\n",
    "                })\n",
    "\n",
    "                model_results['XGBoost'][target_feature] = {\n",
    "                    'y_pred': y_pred_xgb, 'r2': r2_xgb, 'mae': mae_xgb, 'accuracy': acc_xgb\n",
    "                }\n",
    "    wandb.agent(sweep_id, function=train, count=30)\n",
    "\n",
    "\n",
    "# %%capture\n",
    "# param_sweep_xgb(vasp_data, 'VASP')\n",
    "# param_sweep_xgb(lammps_data, 'LAMMPS')\n",
    "# param_sweep_xgb(chroma_data, 'CHROMA')\n",
    "# param_sweep_xgb(espresso_data, 'ESPRESSO')\n",
    "# param_sweep_xgb(atlas_data, 'ATLAS')\n",
    "# param_sweep_xgb(e3sm_data, 'E3SM')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36f6e48-3b8d-4119-8a05-1adeecdaabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_sweep_rf(input_data, dataset_name):\n",
    "    df = input_data.copy()\n",
    "    train_features = ['User', 'JobName', 'Account', 'Category', 'req_node', 'req_time']\n",
    "    cat_cols = ['User', 'JobName', 'Account', 'Category']\n",
    "    num_cols = ['req_node', 'req_time']\n",
    "    target_features = ['gpu_utilization_max', 'mem_util_max', 'avg_power']\n",
    "\n",
    "    results_by_target = {}\n",
    "    model_names = ['RandomForest']\n",
    "    model_results = {model_name: {} for model_name in model_names}\n",
    "\n",
    "    # Define the sweep configuration\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': 'mae',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'n_estimators': {'min': 100, 'max': 1000},\n",
    "            'max_depth': {'min': 2, 'max': 20},       # None for unlimited depth\n",
    "            'min_samples_split': {'min': 2, 'max': 20},\n",
    "            'min_samples_leaf': {'min': 1, 'max': 20},\n",
    "            'max_features': {'values': ['sqrt', 'log2', None]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=f\"{dataset_name}_rf_sweep\")\n",
    "    \n",
    "    def train():\n",
    "        with wandb.init() as run:\n",
    "            # Extract hyperparameters from wandb\n",
    "            config = wandb.config\n",
    "\n",
    "            for target_feature in target_features:\n",
    "                df_cleaned = df.dropna(subset=train_features + [target_feature])\n",
    "                X = df_cleaned[train_features].copy()\n",
    "                y = df_cleaned[target_feature].copy()\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "\n",
    "                # Encode categoricals\n",
    "                encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "                X_train.loc[:, cat_cols] = encoder.fit_transform(X_train[cat_cols])\n",
    "                X_test.loc[:, cat_cols] = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "                # Scale numeric features\n",
    "                scaler = StandardScaler()\n",
    "                X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "                X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "                X_train = X_train.astype(np.float32)\n",
    "                X_test = X_test.astype(np.float32)\n",
    "                y_train = y_train.astype(np.float32)\n",
    "                y_test = y_test.astype(np.float32)\n",
    "\n",
    "\n",
    "                # Train RandomForestRegressor with sweep hyperparameters\n",
    "                reg_rf = RandomForestRegressor(\n",
    "                    random_state=42,\n",
    "                    n_estimators=int(config.n_estimators),\n",
    "                    max_depth=int(config.max_depth) if config.max_depth is not None else None,\n",
    "                    min_samples_split=int(config.min_samples_split),\n",
    "                    min_samples_leaf=int(config.min_samples_leaf),\n",
    "                    max_features=config.max_features,\n",
    "                    bootstrap=True,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                                \n",
    "                reg_rf.fit(X_train, y_train)\n",
    "                y_pred_rf = reg_rf.predict(X_test)\n",
    "                \n",
    "                mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "                r2_rf = r2_score(y_test, y_pred_rf)\n",
    "                acc_rf = np.mean(np.minimum(y_pred_rf / (y_test + 1e-8), y_test / (y_pred_rf + 1e-8)))\n",
    "                \n",
    "                wandb.log({\n",
    "                    'target': target_feature,\n",
    "                    'mae': mae_rf,\n",
    "                    'r2': r2_rf,\n",
    "                    'accuracy': acc_rf\n",
    "                })\n",
    "                \n",
    "                model_results['RandomForest'][target_feature] = {\n",
    "                    'y_pred': y_pred_rf, 'r2': r2_rf, 'mae': mae_rf, 'accuracy': acc_rf\n",
    "                }\n",
    "    wandb.agent(sweep_id, function=train, count=30)\n",
    "\n",
    "\n",
    "# %%capture\n",
    "# param_sweep_rf(vasp_data, 'VASP')\n",
    "# param_sweep_rf(lammps_data, 'LAMMPS')\n",
    "# param_sweep_rf(chroma_data, 'CHROMA')\n",
    "# param_sweep_rf(espresso_data, 'ESPRESSO')\n",
    "# param_sweep_rf(atlas_data, 'ATLAS')\n",
    "# param_sweep_rf(e3sm_data, 'E3SM')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc427cf-09a0-48a0-b8f2-fca6cb9593f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f193b9d-d9dd-4079-9483-b926d1e97cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the parameter sweeps, the best parameters are observed and saved to model_params.json\n",
    "# Results of runs: https://wandb.ai/boztop-boston-university/E3SM_rf_sweep/reports/Paper-experiments--VmlldzoxNTc1NDc0Nw?accessToken=5dnb2oh8hi0ch9trnl9hn8fk5borccd66pidf9vmule1go9vsw9kavkfva1g7zel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
